---
title: 'MLOps is messy at the moment, but the value it can deliver to the society is big'
date: 2023-09-16
permalink: /posts/2023/09/NewKafka/
tags:
  - cool reposts
  - MLOps
---

**[Kafka: The Evolving Data Lake](https://superdatascience.ontraport.com/c/s/73e/UsI6w/U/U6iv/6eRY/6XhWDk/zpyudjssTe/P/P/pC)**

[![](https://i.ontraport.com/135232.56160ab4998565887ada9f2c9f99c9dc.PNG)](https://superdatascience.ontraport.com/c/s/73e/UsI6w/j/U6iv/6eRY/6XhWDk/zlCmm9xV4V/P/P/pC)

**In brief:** There is a transformative shift in data management towards data lakes. This article, highglighted by **SuperDataScience** newsletters, examines how integrating computer engines like Apache Spark, Trino, or ClickHouse, can make data lakes become 'data lakehouses,' enabling efficient data storage and processing. Apache Kafka, a popular event streaming platform, is traditionally used to store recent data before transferring it to data lakes. However, there is evidence suggesting that it is evolving into a new form of data lake. The article explores why Kafka is well-suited for this and also discusses how Kafka can serve as a single source of truth, simplifying data architecture and benefiting from its rich ecosystem for data ingestion and processing.

**Why this is important:**

Understanding how Kafka can serve as a data lake and its potential advantages, such as cost-efficiency and real-time data processing, can inform data storage and processing decisions. Additionally, knowledge of Kafka's limitations and the possibility of hybrid approaches with existing data lake frameworks can help us data scientists design effective data architectures.

Here are some key points from the article:

* Kafka has all the essential properties of a data lake, such as database-like ACID properties, cost-efficient tiered storage, ability to store data of different types, and support for real-time data ingestion.
* Using Kafka as a data lake can simplify the data architecture by eliminating the need to move data between different systems, reducing data inconsistency and loss, and serving as a single source of truth for the entire organization.
* Kafka also has a rich and robust ecosystem for ingesting and processing data from various sources, and most compute engines can readily consume data from Kafka^1^. Moreover, Kafka natively offers lightweight stream processing capabilities through Kafka Streams.
* However, Kafka is not yet ready to replace existing data lake managing frameworks like Apache Iceberg, Apache Hudi, and Delta Lake, which are optimized for large-scale data storage and analysis. Kafka still lacks some crucial features, such as data type awareness for compression, support for query pushdown, and support for updates and deletes.
* A possible future architecture is to use Kafka as a unified interface for reading and writing data, and store hot and warm data in Kafka. Then, cold data can be transparently transitioned from Kafka to Iceberg/Hudi/Delta without the userâ€™s awareness.
* To build a streaming data lakehouse on top of Kafka, one needs two key components: a stream processing system (such as RisingWave, Apache Flink, or KsqlDB) and a real-time analytical engine (such as Apache Spark, Trino, or ClickHouse). These components enable businesses to process and analyze both real-time and historical data stored in Kafka.

*[Click here to read on!](https://superdatascience.ontraport.com/c/s/73e/UsI6w/5/U6iv/6eRY/6XhWDk/sSOpNIVxWK/P/P/pC)*
