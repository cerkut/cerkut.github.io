<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Cumhur Erkut, Dr. Sc. (Tech) </title> <meta name="author" content="Cumhur Erkut"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cerkut.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Cumhur Erkut, Dr. Sc. (Tech) </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/lab/">cuer lab </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IWAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iwai25.png" sizes="200px"></source> <img src="/assets/img/publication_preview/iwai25.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iwai25.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="einstein1950meaning" class="col-sm-8"> <div class="title">Real-time Active Inference for Auditory Hallucination Modeling in VR-based Voice Conversion</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>Proc. Intl. Workshop on Active Inference (IWAI)</em>. Accepted as a poster with Spotlight Presentation, Montreal, Canada , Oct 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://iwaiworkshop.github.io" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/avatars.png" sizes="200px"></source> <img src="/assets/img/publication_preview/avatars.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="avatars.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Bargum-2025" class="col-sm-8"> <div class="title">Exploring the Impact of AI-Generated Speech on Avatar Perception and Realism in Virtual Reality Environments</div> <div class="author"> <a href="https://www.linkedin.com/in/anders-bargum-b887a81a7" rel="external nofollow noopener" target="_blank">Anders R. Bargum</a>, Emil Sønderskov Hansen, <em>Cumhur Erkut</em>, and Stefania Serafin </div> <div class="periodical"> <em>In IEEE Conf. Virtual Reality and 3D User Interfaces - Abstracts and Workshops (VRW)</em>. <em>Video, audio, and data</em> can be <a href="https://drive.google.com/drive/folders/1TFrTbASAWWYpmzL82ohYOUP9JH4VtUi0" rel="external nofollow noopener" target="_blank">found at the html link</a> , Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/vrw66409.2025.00134" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://drive.google.com/drive/folders/1TFrTbASAWWYpmzL82ohYOUP9JH4VtUi0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">DAFx</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dafx24.png" sizes="200px"></source> <img src="/assets/img/publication_preview/dafx24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dafx24.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.48550/arxiv.2408.16546" class="col-sm-8"> <div class="title">RAVE for Speech: Efficient Voice Conversion at High Sampling Rates</div> <div class="author"> <a href="https://www.linkedin.com/in/anders-bargum-b887a81a7" rel="external nofollow noopener" target="_blank">Anders R. Bargum</a>, Simon Lajboschitz, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In Digital Audio Effects Conf. (DAFx)</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.48550/arxiv.2408.16546" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2408.16546" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://rave-for-speech-audio.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/reimagine.png" sizes="200px"></source> <img src="/assets/img/publication_preview/reimagine.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="reimagine.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.3389/frsip.2024.1339159" class="col-sm-8"> <div class="title">Reimagining speech: a scoping review of deep learning-based methods for non-parallel voice conversion</div> <div class="author"> <a href="https://www.linkedin.com/in/anders-bargum-b887a81a7" rel="external nofollow noopener" target="_blank">Anders R. Bargum</a>, Stefania Serafin, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>Frontiers in Signal Processing</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3389/frsip.2024.1339159" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2311.08104" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="badges"> <span class="altmetric-embed" data-badge-type="2" data-badge-popover="right" data-altmetric-id="166386432"></span> <span class="__dimensions_badge_embed__" data-doi="10.3389/frsip.2024.1339159" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=NPYFs5EAAAAJ&amp;citation_for_view=NPYFs5EAAAAJ:BKYZGPsuSFYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios are gaining increasing popularity. Although many of the works in the field of voice conversion share a common global pipeline, there is considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods included when training voice conversion models can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 628 publications from more than 38 venues between 2017 and 2023, followed by an in-depth review of a final database of 130 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls. We condense the knowledge gathered to identify main challenges, supply solutions grounded in the analysis and provide recommendations for future research directions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009fe6"> <div>ACM</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/9vjm6c0k.png" sizes="200px"></source> <img src="/assets/img/publication_preview/9vjm6c0k.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="9vjm6c0k.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kiliboz-2024" class="col-sm-8"> <div class="title">Multimodal Looper: A Live-Looping System for Gestural and Audio-visual Improvisation</div> <div class="author"> Pelin Kiliboz, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In Proceedings of the 9th International Conference on Movement and Computing</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3658852.3659068" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://share.descript.com/view/ypJ08P6tpPo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We present the Multimodal Looper, an embodied digital interface that connects body movements to audiovisual forms for musical improvisation. It extends the performative practice of the musical looper, which enables musicians to record and play back multiple layers of sound in real-time. With the multimodal looper, we explore music cognition from an embodied perspective, considering cross-modal correspondences in order to achieve an intuitive method suitable for collaboration. Our goal is to create a live-looping system with multimodal objects that are gesturally activated and visually represented. Our first prototype focused on the essential modules of a live-looping system: for gesture recognition, we used a depth camera and a decision tree; for visuals that correspond to distinct categories of sounds, e.g., sustained, iterative, and impulsive, we employed procedural generation techniques such as animated noise, feedback loops, instancing, and various mathematical operations. The system’s effectiveness in establishing cross-modal correspondences for a multisensory experience was evaluated through user testing</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">DAFx</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dafx23.png" sizes="200px"></source> <img src="/assets/img/publication_preview/dafx23.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dafx23.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="BAR23-DAFx" class="col-sm-8"> <div class="title">Differentiable Allpass Filters for Phase Response Estimation and Automatic Signal Alignment</div> <div class="author"> Anders R Bargum, Stefania Serafin, <em>Cumhur Erkut</em>, and Julian D Parker </div> <div class="periodical"> <em>In Digital Audio Effects Conf. (DAFx)</em>, May 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arxiv.2306.00860" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2306.00860" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://abargum.github.io" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://github.com/abargum/diff-apf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Virtual analog (VA) audio effects are increasingly based on neural networks and deep learning frameworks. Due to the underlying black-box methodology, a successful model will learn to approximate the data it is presented, including potential errors such as latency and audio dropouts as well as non-linear characteristics and frequency-dependent phase shifts produced by the hardware. The latter is of particular interest as the learned phase-response might cause unwanted audible artifacts when the effect is used for creative processing techniques such as dry-wet mixing or parallel compression. To overcome these artifacts we propose differentiable signal processing tools and deep optimization structures for automatically tuning all-pass filters to predict the phase response of different VA simulations, and align processed signals that are out of phase. The approaches are assessed using objective metrics while listening tests evaluate their ability to enhance the quality of parallel path processing techniques. Ultimately, an over-parameterized, BiasNet-based, all-pass model is proposed for the optimization problem under consideration, resulting in models that can estimate all-pass filter coefficients to align a dry signal with its affected, wet, equivalent.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">DAFx</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dafx23.png" sizes="200px"></source> <img src="/assets/img/publication_preview/dafx23.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dafx23.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SUD23-DAFx" class="col-sm-8"> <div class="title">Vocal Timbre Effects with Differentiable Digital Signal Processing</div> <div class="author"> David Südholt, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In Digital Audio Effects Conf. (DAFx)</em>, May 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arxiv.2306.10886" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2306.10886" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dsuedholt.github.io/ddsp-vocal-effects/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://github.com/dsuedholt/ddsp_xsynth" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We explore two approaches to creatively altering vocal timbre using Differentiable Digital Signal Processing (DDSP). The first approach is inspired by classic cross-synthesis techniques. A pretrained DDSP decoder predicts a filter for a noise source and a harmonic distribution, based on pitch and loudness information extracted from the vocal input. Before synthesis, the harmonic distribution is modified by interpolating between the predicted distribution and the harmonics of the input. We provide a real-time implementation of this approach in the form of a Neutone model. In the second approach, autoencoder models are trained on datasets consisting of both vocal and instrument training data. To apply the effect, the trained autoencoder attempts to reconstruct the vocal input. We find that there is a desirable "sweet spot" during training, where the model has learned to reconstruct the phonetic content of the input vocals, but is still affected by the timbre of the instrument mixed into the training data. After further training, that effect disappears. A perceptual evaluation compares the two approaches. We find that the autoencoder in the second approach is able to reconstruct intelligible lyrical content without any explicit phonetic information provided during training.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pruning.png" sizes="200px"></source> <img src="/assets/img/publication_preview/pruning.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pruning.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1109/taslp.2022.3223257" class="col-sm-8"> <div class="title">Pruning Deep Neural Network Models of Guitar Distortion Effects</div> <div class="author"> David Sudholt, Alec Wright, <em>Cumhur Erkut</em>, and Vesa Välimäki </div> <div class="periodical"> <em>IEEE/ACM Transactions on Audio, Speech and Language Processing</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/taslp.2022.3223257" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://github.com/dsuedholt/PrunedGuitarVA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Deep neural networks have been successfully used in the task of black-box modeling of analog audio effects such as distortion. Improving the processing speed and memory requirements of the inference step is desirable to allow such models to be used on a wide range of hardware and concurrently with other software. In this paper, we propose a new application of recent advancements in neural network pruning methods to recurrent black-box models of distortion effects using a Long Short-Term Memory architecture. We compare the efficacy of the method on four different datasets; one distortion pedal and three vacuum tube amplifiers. Iterative magnitude pruning allows us to remove over 99 of parameters from some models without a loss of accuracy. We evaluate the real-time performance of the pruned models and find that a 3x-4x speedup can be achieved, compared to an unpruned baseline. We show that training a larger model and then pruning it outperforms an unpruned model of equivalent hidden size. A listening test confirms that pruning does not degrade the perceived sound quality, but may even slightly improve it. The proposed techniques can be used to design computationally efficient deep neural networks for processing the sound of the electric guitar in real time.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="10.1007/978-3-031-04021-4_7" class="col-sm-8"> <div class="title">Embodied and Sonic Interactions in Virtual Environments: Tactics and Exemplars</div> <div class="author"> Sophus Béneé Olsen, Emil Rosenlund Høeg, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In Sonic Interactions in Virtual Environments</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-031-04021-4_7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>As the next generation of active video games (AVG) and virtual reality (VR) systems enter people’s lives, designers may wrongly aim for an experience decoupled from bodies. However, both AVG and VR clearly afford opportunities to bring experiences, technologies, and users’ physical and experiential bodies together, and to study and teach these open-ended relationships of enaction and meaning-making in the framework of embodied interaction. Without such a framework, an aesthetic pleasure, lasting satisfaction, and enjoyment would be impossible to achieve in designing sonic interactions in virtual environments (SIVE). In this chapter, we introduce this framework and focus on design exemplars that come from a soma design ideation workshop and balance rehabilitation. Within the field of physiotherapy, developing new conceptual interventions, with a more patient-centered approach, is still scarce but has huge potential for overcoming some of the challenges facing health care. We indicate how the tactics such as making space, subtle guidance, defamiliarization, and intimate correspondence have informed the exemplars, both in the workshop and also in our ongoing physiotherapy case. Implications for these tactics and design strategies for our design, as well as for general practitioners of SIVE are outlined.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Alonso.2021" class="col-sm-8"> <div class="title">Latent Space Explorations of Singing Voice Synthesis using DDSP</div> <div class="author"> Juan Alonso, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In Proc. Sound and Music Computing Conf.</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arxiv.2103.07197" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2103.07197" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://juanalonso.github.io/DDSP-singing-experiments" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://github.com/juanalonso/DDSP-singing-experiments" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Machine learning based singing voice models require large datasets and lengthy training times. In this work we present a lightweight architecture, based on the Differentiable Digital Signal Processing (DDSP) library, that is able to output song-like utterances conditioned only on pitch and amplitude, after twelve hours of training using small datasets of unprocessed audio. The results are promising, as both the melody and the singer’s voice are recognizable. In addition, we present two zero-configuration tools to train new models and experiment with them. Currently we are exploring the latent space representation, which is included in the DDSP library, but not in the original DDSP examples. Our results indicate that the latent space improves both the identification of the singer as well as the comprehension of the lyrics. Our code is available at https://github.com/juanalonso/DDSP-singing-experiments with links to the zero-configuration notebooks, and our sound examples are at https://juanalonso.github.io/DDSP-singing-experiments/ .</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Ganis.2021" class="col-sm-8"> <div class="title">Real-time Timbre Transfer and Sound Synthesis using DDSP</div> <div class="author"> Francesco Ganis, Erik Frej Knudesn, Søren V K Lyster, Robin Otterbein, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'David Südholt, Cumhur Erkut' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proc. Sound and Music Computing Conf.</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.5281/zenodo.5043235" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Neural audio synthesis is an actively researched topic, having yielded a wide range of techniques that leverages machine learning architectures. Google Magenta elaborated a novel approach called Differential Digital Signal Processing (DDSP) that incorporates deep neural networks with preconditioned digital signal processing techniques, reaching state-of-the-art results especially in timbre transfer applications. However, most of these techniques, including the DDSP, are generally not applicable in real-time constraints, making them ineligible in a musical workflow. In this paper, we present a real-time implementation of the DDSP library embedded in a virtual synthesizer as a plug-in that can be used in a Digital Audio Workstation. We focused on timbre transfer from learned representations of real instruments to arbitrary sound inputs as well as controlling these models by MIDI. Furthermore, we developed a GUI for intuitive high-level controls which can be used for post-processing and manipulating the parameters estimated by the neural network. We have conducted a user experience test with seven participants online. The results indicated that our users found the interface appealing, easy to understand, and worth exploring further. At the same time, we have identified issues in the timbre transfer quality, in some components we did not implement, and in installation and distribution of our plugin. The next iteration of our design will address these issues. Our real-time MATLAB and JUCE implementations are available at https://github.com/SMC704/juce-ddsp and https://github.com/SMC704/matlab-ddsp , respectively.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kaspersen.2020l6v" class="col-sm-8"> <div class="title">Generative Choreographies: The Performance Dramaturgy of the Machine</div> <div class="author"> Esbern Kaspersen, Dawid Górny, <em>Cumhur Erkut</em>, and George Palamas </div> <div class="periodical"> <em>In Proc. Intl. Joint Conf. Computer Vision, Imaging and Computer Graphics Theory and Applications (GRAP)</em>, May 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.5220/0008990403190326" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This paper presents an approach for a full body interactive environment in which performers manipulate virtual actors in order to augment a live performance. The aim of this research is to explore the role of generative animation to serve an interactive performance, as a dramaturgical approach in new media. The proposed system consists of three machine learning modules encoding a human’s movement into generative dance, performed by an avatar in a virtual world. First, we provide a detailed description of the technical aspects of the system. Afterwards, we discuss the critical aspects summarized on the basis of dance practice and new media technologies. In the process of this discussion, we emphasize the ability of the system to conform with a movement style and communicate choreographic semiotics, affording artists with new ways of engagement with their audiences.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Serafin.2020.Journal of New Music Research" class="col-sm-8"> <div class="title">Reflections from five years of Sonic Interactions in Virtual Environments workshops</div> <div class="author"> Stefania Serafin, Federico Avanzini, Amalia De Goetzen, <em>Cumhur Erkut</em>, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Michele Geronazzo, Francesco Grani, Niels Christian Nilsson, Rolf Nordahl' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>Journal of New Music Research</em>, May 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1080/09298215.2019.1708413" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>For the past five years, the authors have been running at the IEEE Virtual Reality Conference a Workshop called Sonic Interactions in Virtual Environments (SIVE). The main goal of the workshop series has been to increase among the virtual reality community awareness of the importance of sonic elements when designing multimodal and immersive virtual environments. Starting from this experience, this paper presents a survey of the main active research topics related to sound in virtual and augmented reality (VR/AR), ranging from basic research in spatial audio rendering and sonic interaction design to applications in interactive environments for training, health, rehabilitation, entertainment, and art. Looking at the different research topics emerging from laboratories worldwide, the paper discusses how different research communities can collaborate and benefit from each other in order to increase sound awareness in VR and AR.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kaspersen.2020" class="col-sm-8"> <div class="title">Hydranet: A Real-Time Waveform Separation Network</div> <div class="author"> Esbern Torgard Kaspersen, Tsampikos Kounalakis, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, May 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/icassp40776.2020.9053357" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Real-time source separation has become increasingly important, as more and more applications, such as voice recognition and voice commands, require clean audio input in noisy environments. Recent developments in deep learning have allowed models to directly exploit the waveform of the audio, making real-time separation achievable. In this paper, we propose a 1-D convolutional U-Net structure to separate waveform input. This structure incorporates recurrent layers, to exploit longer temporal connections in the audio signal. Our proposed network architecture is also benefiting from the addition of an extra output channel, measuring the distortion of the other output channels. Our proposed methodology is experimentally shown to yield state-of-the-art results, using only 0.76 seconds of input audio.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Bigoni.2020" class="col-sm-8"> <div class="title">DogDog</div> <div class="author"> Francesco Bigoni, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>Proceedings of the 7th International Conference on Movement and Computing</em>, May 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3401956.3404242" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Improvisation is embodied thought and expression. This paper outlines strategies and tactics to design expressive musical interfaces for improvisers. Some of these strategies are explored through a case study: a non-tactile hand-arm movement interface controlling a granular synthesizer (DogDog), based on high-level movement descriptors. The research through design and performance experience indicates that movement quality descriptors are inherently scalable from hand-arm movements to full body interaction, and that a textural approach to motion tracking fits well the morphing sonic masses generated through granular sound synthesis.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Andersson.2019" class="col-sm-8"> <div class="title">Immersive Audio Programming in a Virtual Reality Sandbox</div> <div class="author"> Nikolaj Andersson, <em>Cumhur Erkut</em>, and Stefania Serafin </div> <div class="periodical"> <em>In Proc. AES Conf. Immersive and Interactive Audio</em>, Mar 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Immersive sandboxes for audio-visual content creation in Virtual Reality (VR) are becoming widely available, thanks to VR distribution platforms such as Steam VR. Some of these sandboxes are specially build to host VirtualReality Musical Instruments (VRMIs) In this paper, after describing the MuX, a VRMI-hosting sandbox, and its components, we present new elements developed for the environment. We focus on lumped and distributed physically-inspired models for sound synthesis. A simple interface was developed to control the physical models with gestures, expanding the interaction possibilities within MuX. A preliminary evaluation of the sandbox shows that as the number and complexity of the components increase, it becomes important to provide to the users ready-made machines instead of allowing them to build everything from scratch.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut.2019.Journal of Somaesthetics" class="col-sm-8"> <div class="title">Incorporating Virtual Reality with Experiential Somaesthetics in an Embodied Interaction Course</div> <div class="author"> <em>Cumhur Erkut</em>, and Sofia Dahl </div> <div class="periodical"> <em>Journal of Somaesthetics</em>, Mar 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Engagement with virtual reality (VR) through movement is becoming increasingly important. Therefore, VR developers should improve their bodily skills and learn how to use movement as design material. To do so, first-person accounts of the development and experience are necessary. Since these qualities are well addressed in experiential somaesthetics, we explore the education space in VR, with attention to the first-person experiences, movement data, and code. We present an approach for teaching and designing VR-based embodied interaction and describe simple projects implemented by the participants. The evaluation of projects indicates that the concepts, practices, and perspectives of embodied interaction were attained in VR. Our reflections contribute to the literature on movement-based interaction education in VR, and its evaluation and validation by first-person accounts, in addition to the data and program code produced. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Hussain.2019" class="col-sm-8"> <div class="title">Evaluating movement qualities with visual feedback for real-time motion capture</div> <div class="author"> Aishah Hussain, Camilla Modekjaer, Nicoline Warming Austad, Sofia Dahl, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Cumhur Erkut' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> Mar 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/3347122.3347123" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Pajala-Assefa.2019" class="col-sm-8"> <div class="title">A Study of Movement-Sound within Extended Reality</div> <div class="author"> Hanna Pajala-Assefa, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em></em> Mar 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/3347122.3359604" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Baldwin.2019.SMC Nordic Conf." class="col-sm-8"> <div class="title">Efficient Rendering and Perception of Acoustical Environments in Augmented Reality Audio</div> <div class="author"> Alex Baldwin, Jonas Holfelt, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>SMC Nordic Conf.</em>, Mar 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Eklund.2020" class="col-sm-8"> <div class="title">A Positional Infrared Tracking System Using Non-individualised HRTFs to Simulate a Loudspeaker Setup and Its Influence on Externalisation of Music</div> <div class="author"> Rasmus Eklund, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In Proc. EAI Intl. Conf. ArtsIT</em>, Mar 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-030-53294-9_11" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Many artists produce and mix their virtual reality, game, or screen media audio productions only with headphones, but deploy them to stereo or multi-channel loudspeaker setups. Because of the acoustical and perceptual differences, listening on headphones might sound very different compared to loudspeakers, including the perception of sound sources inside the head (externalisation problem). Nevertheless, by using Head Related Transfer Functions (HRTFs) and accurate movement tracking, it is possible to simulate a loudspeaker setup with proper externalisation. In this paper, an infrared-based positional tracking system with non-individualised HRTFs to simulate a loudspeaker setup is conceptualised, designed and implemented. The system can track the user with six degrees of freedom (6-DOF); an improvement over current commercial systems that only use 3-DOF tracking. The system was evaluated on 20 participants to see if the additional DOF increased the degree of externalisation. While tracking increased the externalisation in general, there was no significant difference between 3-DOF and 6-DOF. Another test indicated that positional movement coupled with positional tracking may have a greater effect on externalisation compared to positional movement coupled with only head movement tracking. Comparisons between these results and previous studies are discussed and improvements for future experiments are proposed.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Hoeoek-2018" class="col-sm-8"> <div class="title">Embracing First-Person Perspectives in Soma-Based Design</div> <div class="author"> Kristina Höök, Baptiste Caramiaux, <em>Cumhur Erkut</em>, Jodi Forlizzi, and <span class="more-authors" title="click to view 18 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '18 more authors' ? 'Nassrin Hajinejad, Michael Haller, Caroline Hummels, Katherine Isbister, Martin Jonsson, George Khut, Lian Loke, Danielle Lottridge, Patrizia Marti, Edward Melcer, Florian Müller, Marianne Petersen, Thecla Schiphorst, Elena Segura, Anna Ståhl, Dag Svanæs, Jakob Tholander, Helena Tobiasson' : '18 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">18 more authors</span> </div> <div class="periodical"> <em>Informatics</em>, Mar 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3390/informatics5010008" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>A set of prominent designers embarked on a research journey to explore aesthetics in movement-based design. Here we unpack one of the design sensitivities unique to our practice: a strong first person perspective-where the movements, somatics and aesthetic sensibilities of the designer, design researcher and user are at the forefront. We present an annotated portfolio of design exemplars and a brief introduction to some of the design methods and theory we use, together substantiating and explaining the first-person perspective. At the same time, we show how this felt dimension, despite its subjective nature, is what provides rigor and structure to our design research. Our aim is to assist researchers in soma-based design and designers wanting to consider the multiple facets when designing for the aesthetics of movement. The applications span a large field of designs, including slow introspective, contemplative interactions, arts, dance, health applications, games, work applications and many others.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="serafin-2018-sonic-inter" class="col-sm-8"> <div class="title">Sonic Interactions in Virtual Reality</div> <div class="author"> Stefania Serafin, Michele Geronazzo, <em>Cumhur Erkut</em>, Niels C. Nilsson, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Rolf Nordahl' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Computer Graphics and Applications</em>, Mar 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/mcg.2018.193142628" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>A high-fidelity but efficient sound simulation is an essential element of any VR experience. Many techniques used in virtual acoustics are graphical rendering techniques suitably modified to account for sound generation and propagation. In recent years, several advances in hardware and software technologies have been facilitating the development of immersive interactive sound-rendering experiences. In this article, we present a review of the state of the art of such simulations, with a focus on the different elements that, combined, provide a complete interactive sonic experience. This includes physics-based simulation of soundeffects and their propagation in space together with binaural rendering to simulate the position of sound sources. We present how these different elements of the sound design pipeline have been addressed in the literature, trying to find the trade-off between accuracy and plausibility. Recent applications and current challenges are also presented.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Mandanici:2018ts" class="col-sm-8"> <div class="title">Movement Patterns in the Harmonic Walk Interactive Environment</div> <div class="author"> Marcella Mandanici, <em>Cumhur Erkut</em>, Razvan Paisa, and Stefania Serafin </div> <div class="periodical"> <em>Journal of New Music Research</em>, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The online platform for scientific writing. Overleaf is free: start writing now with one click. No sign-up required. Great on your iPad.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:0a22" class="col-sm-8"> <div class="title">Mobile AR In and Out: Towards Delay-Based Modeling of Acoustic Scenes</div> <div class="author"> <em>Cumhur Erkut</em>, Jonas Holfelt, and Stefania Serafin </div> <div class="periodical"> <em>2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</em>, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/vr.2018.8446230" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We have previously presented an augmented reality (AR) audio application, where scattering delay networks efficiently generate and organize a reverberator, based on room geometry scanned by an AR device. The application allowed for real-time processing and updating of reflection path geometry and provided a proof-of-concept for plausible audio-spatial registration of a virtual object in real environments. Here we present our ongoing work that aims to extend the simulation to outdoor scenes by using the Waveguide Web, instead of the original formulation with the Scattering Delay Networks. The current implementation is computationally more demanding, but has a potential to provide more accurate second-order reflections, and therefore, better registering of audio-visual AR scenes.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Baldwin:2018a22" class="col-sm-8"> <div class="title">Towards the Design and Evaluation of Delay-based Modeling of Acoustic Scenes in Mobile Augmented Reality</div> <div class="author"> Alex Baldwin, Stefania Serafin, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>2018 IEEE 4th VR Workshop on Sonic Interactions for Virtual Environments (SIVE)</em>, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/sive.2018.8577129" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>ScatAR is an augmented reality (AR) audio application, where scattering delay networks (SDNs) efficiently generate and organize a reverberator based on room geometry scanned by an AR device. Here we present a perceptual evaluation of the application in terms of auditory object presence. Specifically, we investigate the personal preferences on the auditory registration of a sounding object (a drone transmitting speech) in a room in anechoic versus artificial reverberation conditions. The results indicate a marginal preference of the reverberation, but much less than what we have anticipated. We discuss possible reasons for this, and set to compare our SDN implementation with a Waveguide Web (WGW) simulation in AR. While the WGWs are computationally more demanding, they have a potential to provide more accurate second-order reflections, and therefore, better registering of audio-visual AR scenes.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut.2018ytr" class="col-sm-8"> <div class="title">Incorporating Virtual Reality in an Embodied Interaction Course</div> <div class="author"> <em>Cumhur Erkut</em>, and Sofia Dahl </div> <div class="periodical"> <em>In </em>, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3212721.3212884" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Engagement with virtual reality (VR) through movement is becoming increasingly important. Therefore, the VR developers should improve their bodily skills and learn how to use the movement as design material. In addition, first person accounts of the development and experience are necessary. We explore the education space in VR with attention to the first-person experiences, movement data and code, and present an approach for teaching and designing VR-based embodied interaction.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2017ke" class="col-sm-8"> <div class="title">Rhythmic interaction in VR: interplay between sound design and editing</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, May 2017 </div> <div class="periodical"> Bump for RAction </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/sive.2017.7901611" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Cinematic virtual reality is a new and relatively unexplored area in academia. While research in guiding the spectator’s attention in this new medium has been conducted for some time, a focus on editing in conjunction with spectator orientation is only currently emerging. In this paper, we consider rhythm as an important element in guiding attention. Starting with the possibility of applying some concepts from rhythm-action games to virtual reality, we discuss specific film editing and rhythmic interaction design techniques that can be used in cinematic virtual reality. We provide a background in rhythm perception, and complement it with applications in traditional editing. Through the notion of multimodal listening we provide guidelines that can be used in rhythmic and sonic interaction design in VR.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Pirhonen:2017ix" class="col-sm-8"> <div class="title">Human-Technology Choregraphies: Body, Movement, and Space in Expressive Interactions</div> <div class="author"> Antti Pirhonen, Kai Tuuri, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>Human Technology</em>, May 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.17011/ht/urn.201705272515" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Stovring:2016ir" class="col-sm-8"> <div class="title">Multi-kinect Skeleton Fusi on for Enactive Games</div> <div class="author"> Nikolaj Marimo Støvring, Esbern Torgard Kaspersen, Jeppe Milling Korsholm, Yousif Ali Hassan Najim, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Soraya Makhlouf, Alireza Khani, Cumhur Erkut' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In </em>, May 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-319-55834-9_20" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We present a procedural method and an implementation of multi-Kinect skeleton fusion on Unity environment. Our method calibrates two Kinects by combining the relative coordinates of a user’s torso onto a single coordinate system. The method is tested with a small number of participants in scenarios involving multiple users, results indicate that the method provides an improvement over a single camera, and it is accurate enough for games and entertainment applications. The video demonstration of the system is provided, and future directions to improve accuracy are outlined.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2017cc" class="col-sm-8"> <div class="title">Embodied Interaction through Movement in a Course Work</div> <div class="author"> <em>Cumhur Erkut</em>, and Sofia Dahl </div> <div class="periodical"> <em>In </em>, Jun 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/3077981.3078026" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2017vg" class="col-sm-8"> <div class="title">Microphone-based Electronic Wind Instrument Using Feature Extraction</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> <em></em> Dec 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Baldwin:2017f31" class="col-sm-8"> <div class="title">ScatAR: a mobile augmented reality application that uses scattering delay networks for room acoustic synthesis</div> <div class="author"> Alex Baldwin, Stefania Serafin, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology</em>, Dec 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3139131.3141201" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We present an augmented reality (AR) audio application where scattering delay networks efficiently generate and organize a reverberator, based on room geometry scanned by an AR device. The application allows for real-time processing and updating of reflection path geometry. It provides a proof-of-concept for plausible audio-spatial registration of a virtual object in a real environment, but further tests are needed in perceptual evaluation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut.2017" class="col-sm-8"> <div class="title">Structuring Design and Evaluation of an Interactive Installation Through Swarms of Light Rays with Human-Artifact Model</div> <div class="author"> <em>Cumhur Erkut</em>, and Jonas Fehr </div> <div class="periodical"> <em>In </em>, May 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-319-55834-9_5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We present the design and evaluation of an interactive installation to be explored by movement and sound under Human-Activity Model. In the installation, movement qualities that are extracted from the motion tracking data excite a dynamical system (a synthetic flock of agents), which responds to the movement qualities and indirectly controls the visual and sonic feedback of the interface. In other words, the relationship between gesture and sound are mediated by synthetic swarms of light rays. A test session was conducted with eleven subjects, who were asked to investigate the installation and to fill out a questionnaire afterwards. In this paper, we report our preliminary work on the analysis of the tensions of interaction with the installation under the Human-Artifact Model. Our results indicate exploration and discovery as the main motives of the interaction. This is different than utilitarian HCI artifacts, where the instrumental aspects are typically in the foreground.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Overholt:2016ue" class="col-sm-8"> <div class="title">Drive of Musical HCI: Design and Evaluation Concerns For Real-Time Interactions</div> <div class="author"> Dan Overholt, and <em>Cumhur Erkut</em> </div> <div class="periodical"> Jan 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Serafin:2016vf" class="col-sm-8"> <div class="title">Virtual reality and the senses</div> <div class="author"> Stefania Serafin, Niels Christian Nilsson, <em>Cumhur Erkut</em>, and R Nordahl </div> <div class="periodical"> Nov 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2016wi" class="col-sm-8"> <div class="title">Movement and Coding in an Embodied Interaction Course</div> <div class="author"> <em>Cumhur Erkut</em>, and Sofia Dahl </div> <div class="periodical"> Jan 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Movement-based design is reaching critical mass in HCI, and we can start to identify strategies, similarities and differences in how it is approached. Similarities may include, for example, a strong first person perspective on design, emphasising movement, somatics and aesthetic sensibilities of the designer, as well as starting from the premise that our bodily ways of being in the world are shaped by the ecologies of people, cultural practices and the artefacts we create and use. The workshop will discuss similarities and differences between specific design exemplars; ways of extending on our senses and perception – even creating new senses through technology; social interactions, engaging us to jointly explore movement or touch; endowing machines with their own ‘somatics’; as well as engaging in larger political issues around the body, such as gender perspectives, or challenging the mind-body divide.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Hagensen:2016tk" class="col-sm-8"> <div class="title">An Experimental Study in Generative Music for Exercising to Ease Perceived Exertion by use of Heart Beat Rate as a Control Parameter</div> <div class="author"> Troels Lunde Hagensen, Stefania Serafin, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, Mar 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Serafin.2016.AM" class="col-sm-8"> <div class="title">Virtual reality musical instruments: Guidelines for multisensory interaction design</div> <div class="author"> Stefania Serafin, <em>Cumhur Erkut</em>, Juraj Kojs, Rolf Nordahl, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Niels C. Nilsson' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In </em>, Mar 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2986416.2986431" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The rapid development and availability of low cost technologies has created a wide interest in virtual reality (VR), but how to design and evaluate multisensory interactions in VR remains as a challenge. In this paper, we focus on virtual reality musical instruments, present an overview of our design and evaluation guidelines, and examine historical case studies. Our main contribution is to inform the design and evaluation of the future VRMIs and consider the challenges.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Maculewicz:2015gn" class="col-sm-8"> <div class="title">An investigation on the impact of auditory and haptic feedback on rhythmic walking interactions</div> <div class="author"> Justyna Maculewicz, <em>Cumhur Erkut</em>, and Stefania Serafin </div> <div class="periodical"> <em>International Journal of Human-Computer Studies</em>, Mar 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.ijhcs.2015.07.003" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This paper presents a system capable of rhythmic walking interaction by auditory and haptic display. Likewise, it summarizes the results of the research on the influence of the audio and haptic stimulation on rhythmic walking interaction. The system detects user’s footsteps and either provides interactive real-time feedback or suggests a pace using a synthetic walking sound or vibration. This pace is either a constant tempo or adapts to the walker. Auditory and haptic feedback signals are either ecological physically-based synthetic walking signals or simple sinusoidal beeps. In the experiment, the different auditory and haptic feedback and interaction modes are studied with respect to their effect on the walking tempo. The results show that participants synchronise equally well with the tempo with either audio or haptic cues, but indicate the audio–haptic conditions as the easiest to synchronise with. Moreover, results indicate that multimodal audio–haptic feedback provide the most natural feeling. These results have implications on the design of interactive entertainment or therapeutical applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Pirhonen:2016fe" class="col-sm-8"> <div class="title">Human-Technology Choreographies: Body, Movement, and Space</div> <div class="author"> Antti Pirhonen, Kai Tuuri, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>Human Technology</em>, Mar 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.17011/ht/urn.201605192617" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Maculewicz:2016us" class="col-sm-8"> <div class="title">How can soundscapes affect the preferred walking pace?</div> <div class="author"> Justyna Maculewicz, <em>Cumhur Erkut</em>, and Stefania Serafin </div> <div class="periodical"> <em>Applied Acoustics</em>, Mar 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1016/j.apacoust.2016.07.031" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Anonymous:2016wg" class="col-sm-8"> <div class="title">Electronic Armonica: a Tangible Musical Interface inspired by the Glass Harmonica</div> <div class="author"> Razvan Paisa, <em>Cumhur Erkut</em>, and Stefania Serafin </div> <div class="periodical"> Oct 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2986416.2986434" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This paper describes the design, implementation and evaluation of a tangible musical interface inspired by the glass harmonica, and two methods for synthesizing a glass sound: additive synthesis and physical modeling using banded waveguides. These methods were implemented in Pure Data and in Max/MSP, respectively. An interface was created using laser cutting and 3D printing tools. An Arduino microcontroller is used to create a virtual capacitive sensor for detecting touch. The interface has been evaluated in order to test its playability and to evaluate the overall behavior of the system. Besides having technical limitations expected from a prototype, the results are promising.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2016ic" class="col-sm-8"> <div class="title">From Ecological Sounding Artifacts Towards Sonic Artifact Ecologies</div> <div class="author"> <em>Cumhur Erkut</em>, and Stefania Serafin </div> <div class="periodical"> <em>In </em>, May 2016 </div> <div class="periodical"> Not moving. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2851581.2892583" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The discipline of sonic interaction design has been focused on the interaction between a single user and an artifact. This strongly limits one of the fundamental aspects of music as a social and interactive experience. In this paper we propose sonic artifact ecologies as a mean to examine interactions between one or many users with one or many artifacts. Case studies from a recently run workshop on product sound design are examined.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2016ed" class="col-sm-8"> <div class="title">A Case of Cooperative Sound Design</div> <div class="author"> <em>Cumhur Erkut</em>, Davide Rocchesso, Stefano Delle Monache, and Stefania Serafin </div> <div class="periodical"> <em>In </em>, May 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2971485.2996472" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In this design case study, protocol and linkographic analysis are applied to a task of cooperative vocal sketching, proposed in the scope of educational research activities. The understanding of the cognitive behaviors involved in sound creation is aimed at setting the ground for the development of rigorous, designerly evaluation practices tailored to sound design, all the way to the final interactive product. Relevant qualitative and quantitative information about the creative process informs the assessment and possibly improvement of sound design methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Serafin:2016kh" class="col-sm-8"> <div class="title">Virtual Reality Musical Instruments: State of the Art, Design Principles, and Future Directions</div> <div class="author"> Stefania Serafin<sup>*</sup>, <em>Cumhur Erkut<sup>*</sup></em>, Juraj Kojs<sup>†</sup>, Niels C. Nilsson<sup>*</sup>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Rolf Nordahl&lt;sup&gt;*&lt;/sup&gt;' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Computer Music Journal</em>, Sep 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1162/comj_a_00372" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The rapid development and availability of low-cost technologies have created a wide interest in virtual reality. In the field of computer music, the term “virtual musical instruments” has been used for a long time to describe software simulations, extensions of existing musical instruments, and ways to control them with new interfaces for musical expression. Virtual reality musical instruments (VRMIs) that include a simulated visual component delivered via a head-mounted display or other forms of immersive visualization have not yet received much attention. In this article, we present a field overview of VRMIs from the viewpoint of the performer. We propose nine design guidelines, describe evaluation methods, analyze case studies, and consider future challenges.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Grani:2015ul" class="col-sm-8"> <div class="title">Spatial Sound and Multimodal Interaction in Immersive Environments</div> <div class="author"> Francesco Grani, Dan Overholt, <em>Cumhur Erkut</em>, Steven Gelineck, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Georgios Triantafyllidis, R Nordahl, Stefania Serafin' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In </em>, Sep 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2814895.2814919" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Spatial sound and interactivity are key elements of investigation at the Sound And Music Computing master program at Aalborg University Copenhagen. We present a collection of research directions and recent results from work in these areas, with the focus ...</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Fehr:2015vk" class="col-sm-8"> <div class="title">LichtGestalt: Interaction with Sound Through Swarms of Light Rays</div> <div class="author"> Jonas Fehr, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, Jun 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2014tr" class="col-sm-8"> <div class="title">Arts and Technology</div> <div class="author"> <em>Cumhur Erkut</em>, Anu Rajala-Erkut, and Sofia Dahl </div> <div class="periodical"> <em>Arts and Technology</em>, Jun 2015 </div> <div class="periodical"> #Dance #embodied interaction </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-319-18836-2_10" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We present approaches for teaching and designing embodied interaction in collaboration with a contemporary dance choreographer. Our approaches are based on the felt qualities of movement, providing a shared experience, vocabulary for self-expression, and appreciation for movement as a design material for interaction design practitioners. In parallel, such activities provide art professionals competencies for new contexts. We present two workshops conducted at different times. The first workshop, back in 2009, brought about novel sonic interaction paradigms, technologies, and artifacts. The second workshop was carried out in March 2014, and we are in the process of developing interactive sketches by pairing our observations with motion tracking. In this paper, the activities in these workshops are presented, and reflected upon. In particular, we are investigating whether or not these activities guided the participants from the prevailing notion of command/control in embodied interaction towards experiences related to the felt qualities of movement.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Serafin:2015ej" class="col-sm-8"> <div class="title">The digital Intonarumori</div> <div class="author"> Stefania Serafin, Amalia De Gotzen, Smilen Dimitrov, Steven Gelineck, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Cumhur Erkut, Niels Christian Nilsson, Francesco Grani, R Nordahl, Stefano Trento' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In </em>, Jun 2015 </div> <div class="periodical"> Points of departure: - Continous control - Leap Motion Virtualization - Commuted synth </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/3dui.2015.7131773" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The Intonarumori (Noise Intoners) were a family of musical instruments designed and built by the Italian composer and painter Luigi Russolo at the beginning of the 20th century (see Figure 1). Each Intonarumori was made of a colorful parallelepipedal sound box with a </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Maculewicz:2015uk" class="col-sm-8"> <div class="title">Rhythmic Walking Interaction with Auditory Feedback: Ecological Approaches in a Tempo Following Experiment</div> <div class="author"> Justyna Maculewicz, Antti Jylha, Stefania Serafin, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>IEEE MultiMedia</em>, Jun 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/mmul.2015.27" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This study presents a system capable of rhythmic walking interactions by auditory display. The feedback is based on footstep sounds, and follows either detected footsteps, or suggests a tempo, which is either constant or adapts to the walker. The auditory display contains simple sinusoidal tones or ecological, physically-based synthetic walking sounds. In the tempo-following experiment, we investigate the different interaction modes (step versus constant or adaptive tempo) and auditory feedback (sinusoidal tones versus ecological walking sounds) with respect to their effect on the walking tempo. Quantitatively, we calculate the mean square error (MSE) between the performed and target tempo, and the stability of the performed tempo. The results indicate that the MSE with ecological sounds is better or comparable to that with the sinusoidal tones, yet ecological sounds are considered more natural. Allowing deviations from the cues in the adaptive conditions results in a tempo that is still stable, but closer to the natural walking pace of the subjects. These results have implications on the design of interactive entertainment or rehabilitation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Fehr:2015ke" class="col-sm-8"> <div class="title">Indirection between movement and sound in an interactive sound installation</div> <div class="author"> Jonas Fehr, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, Aug 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2790994.2791016" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We present a new interactive sound installation to be explored by movement, specifically by the movement qualities extracted from the motion tracking data. There is an indirection between movement and sound: movement qualities control a dynamical system (in our case a flock of agents), which in turn controls the visual and sonic feedback of the interface. The movement qualities are extracted by simple measures. The system is implemented, evaluated, and will be demonstrated during MOCO’15.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Serafin:2015fh" class="col-sm-8"> <div class="title">Sonic interaction in virtual environments</div> <div class="author"> Stefania Serafin, R Nordahl, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>2015 IEEE 2nd VR Workshop on Sonic Interactions for Virtual Environments (SIVE)</em>, Aug 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/sive.2015.7361283" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This paper summarizes the main research topics addressed at the 2<sup>nd</sup> workshop on Sonic Interaction in Virtual Environments (SIVE) that took place in Arles, France in March 2015. The workshop is part of the annual IEEE Virtual Reality Confer...</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2015dc" class="col-sm-8"> <div class="title">Product Sound Design: Form, Function, and Experience</div> <div class="author"> <em>Cumhur Erkut</em>, Stefania Serafin, Michael Hoby, and Jonniy Sårde </div> <div class="periodical"> <em>In </em>, Aug 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2814895.2814920" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Current interactive products, services, and environments are appraised by their sensory attributes, in addition to their form and function. Sound is an important factor in these multisensory product appraisals. Integrating this sound opportunity into the design and development of interactive products, which are fit for real-world, yet constitute a strong brand identity, remains a challenge. We address this challenge by applying the research know-how of an academic institution and business practices of a sound agency SME within the core R&amp;D and production process of the third industrial partner. Our approach has clear application scenarios in, e.g., extended wireless headsets, car audio appliances, and portable entertainment devices. We describe the prototypes developed during the project life span, and the activities and outcomes of a half-day workshop designed to disseminate the project results.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Maculewicz:2015vh" class="col-sm-8"> <div class="title">An investigation on the influence of soundscapes and footstep sounds in affecting preferred walking pace</div> <div class="author"> Justyna Maculewicz, <em>Cumhur Erkut</em>, and Stefania Serafin </div> <div class="periodical"> <em>In </em>, Aug 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this paper we describe an experiment whose goal is to investigate the role of footstep sounds and soundscapes to affect the pace of a person who is walking in place, eg, mimicking the act of walking without leaving the current position. The results of a ... </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kirwan:2015tx" class="col-sm-8"> <div class="title">Bean: A Digital Musical Instrument for Use in Music Therapy</div> <div class="author"> Nicholas J Kirwan, Dan Overholt, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, Jul 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.5281/zenodo.851069" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>(Abstract to follow)</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Conte:E7CGxjBi" class="col-sm-8"> <div class="title">Beyond Command &amp; Control: Sketching Embodied Interaction</div> <div class="author"> <em>Cumhur Erkut</em>, and Anu Rajala-Erkut </div> <div class="periodical"> <em>In </em>, Jul 2015 </div> <div class="periodical"> #one gourd bigger than the other </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2702613.2732855" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We present an approach for teaching and designing embodied interaction in collaboration with a contemporary dance choreographer. Our approach is based on the felt qualities of movement, and provides a shared experience, vocabulary for self-expression, and appreciation for movement as a design material for interaction design practitioners. We present a workshop, where after movement sessions, interactive sketches were generated and implemented by motion tracking. Subsequently, we have investigated whether or not these activities guided the participants from the prevailing notion of command/control in embodied interaction towards experiences related to the felt qualities of movement. While in idea generation, our approach has provided a better foundation for participants, compared to the approaches that focus only on technologies, this effect wore off and final implementations focused on command &amp; control. We currently experiment with new tools and techniques, integrating material interactions into the process.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Maculewicz:2015a22" class="col-sm-8"> <div class="title">The Effects of Ecological Auditory Feedback on Rhythmic Walking Interaction</div> <div class="author"> Justyna Maculewicz, Antti Jylha, Stefania Serafin, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>IEEE MultiMedia</em>, Jul 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/mmul.2015.17" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The authors present a system capable of rhythmic walking interactions using auditory display. The feedback is based on footstep sounds and follows either detected footsteps or suggests a tempo, which is either constant or adapts to the walker. The auditory display contains simple sinusoidal tones or ecological, physically based synthetic walking sounds. In the tempo-following experiment, the authors investigate the different interaction modes (step versus constant or adaptive tempo) and auditory feedback (sinusoidal tones versus ecological walking sounds) with respect to their effect on walking tempo. They calculate the mean square error (MSE) between the performed and target tempo and the stability of the performed tempo. The results indicate that the MSE with ecological sounds is better than or comparable to that with the sinusoidal tones, yet ecological sounds are considered more natural. Allowing deviations from the cues in the adaptive conditions results in a tempo that’s still stable but closer to the natural walking pace of the subjects. These results have implications on the design of interactive entertainment or rehabilitation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Summers:2015wd" class="col-sm-8"> <div class="title">Sonic Interaction Design for Virtual and Augmented Reality Environments</div> <div class="author"> Chanel Summers, Vangelis Lympouridis, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, Jul 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/sive.2015.7361290" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Sonic Interaction Design (SID) primarily focuses on enhancing the quality of an interactive experience for the user by utilizing explicit techniques and defining advanced computational objects. While the diverse methods in SID are being documented, we argue for a need to refocus on the experiential, embodied, contextual, playful, and holistic qualities of sonic interactions. In this paper, we discuss specific SID techniques that can be used to advance story and gameplay in virtual and augmented environments. We shortly introduce the audio technologies used in virtual reality, and complement these with the practical examples about their (mis)use. We then illustrate the SID in the augmented environment of Leviathan, featured in Intel’s 2014 CES keynote. We discuss methods such as defining characters through sound, embracing and critically using ambiguity, and coordinating sound and music. Through the notion of multimodal listening we provide guidelines to sonic interaction designers, so that they not only make their audience to listen in to digital content, but also listen up, out, through, and around.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Serafin:2014wn" class="col-sm-8"> <div class="title">Sound and Music Computing at Aalborg University in Copenhagen</div> <div class="author"> Stefania Serafin, Sofia Dahl, Amalia De Gotzen, <em>Cumhur Erkut</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Dan Overholt, Hendrik Purwins, B Sturm' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In </em>, Sep 2014 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Fontana:2014wj" class="col-sm-8"> <div class="title">Piano from Nothing</div> <div class="author"> Federico Fontana, Stefania Serafin, and <em>Cumhur Erkut</em> </div> <div class="periodical"> Sep 2014 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2014hm" class="col-sm-8"> <div class="title">Design and evaluation of interactive musical fruit</div> <div class="author"> <em>Cumhur Erkut</em>, Stefania Serafin, Jonas Fehr, Henrique M.R. Fernandes Figueira, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Theis B. Hansen, Nicholas J. Kirwan, Mariam R. Zakarian' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> Jun 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2593968.2610451" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In this paper we describe the design and evaluation of a novel, tangible user interface for interaction with sound, to be implemented in a museum setting. Our workinprogress is part of a larger concept for an installation prioritizing a collaborative, explorative, multimodal experience. Focus has been centered on novice children, in order to accommodate all potential users of the museum, and to minimize the risk of excluding users based on skill or previous musical knowhow. We have developed four instances of a multimodal device for interacting with sounds via a tangible interface, and called them Interactive Musical Fruits (IMFs). The IMF consists of an embedded processing system, which can detect its orientation. Qualitative testing with children has been performed, to better evaluate the current design state. Positive feedback from the test subjects upholds the validity and the potential of the IMF as an interface in a museum context. However, further research is required to improve the interactive and collaborative aspects of the device, as well as the aural and visual properties of the IMF.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2013</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2013wx" class="col-sm-8"> <div class="title">Apps for Me, Too: Gaze and Natural Interaction Applications for Varying Cognitive Strengths on a Multi-touch Table Network</div> <div class="author"> <em>Cumhur Erkut</em>, Roman Bednarik, Susanne Mäkelä, and Jari Kleimola </div> <div class="periodical"> <em>In </em>, Apr 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2013uk" class="col-sm-8"> <div class="title">Heigh Ho: Rhythmicity in sonic interaction</div> <div class="author"> <em>Cumhur Erkut</em>, Antti Jylhä, and Davide Rocchesso </div> <div class="periodical"> <em>In </em>, Apr 2013 </div> <div class="periodical"> #iPalmas #Product sounds #PRSD #psd #rhythmicity </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2013ta" class="col-sm-8"> <div class="title">(and Sound) of SiMPE: Showcasing Outcomes of a Mobile Audio Programming Seminar</div> <div class="author"> <em>Cumhur Erkut</em>, Antti Jylhä, and Stefania Serafin </div> <div class="periodical"> <em>In </em>, Jul 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jylha:2013vq" class="col-sm-8"> <div class="title">I Can Hear You: Private, Public, and Social Sonic Interactions in Public Spaces</div> <div class="author"> Antti Jylhä, <em>Cumhur Erkut</em>, and Giulio Jacucci </div> <div class="periodical"> <em>In </em>, Apr 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Serafin:2013vs" class="col-sm-8"> <div class="title">Spatialized audio haptic feedback for training and rehabilitation</div> <div class="author"> Stefania Serafin, and <em>Cumhur Erkut</em> </div> <div class="periodical"> Apr 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2013uh" class="col-sm-8"> <div class="title">Mobile Probes for Special Needs</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> <em></em> Apr 2013 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>ABSTRACT At our institute we currently run several projects in a Technology-enhanced Learning Environment (TEL) with and for the children with special needs. Families participate as experts in the development and evaluation process by sharing feedback ... </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2013vw" class="col-sm-8"> <div class="title">Rhythm-Action Games: The Sonic Interaction Perspective</div> <div class="author"> <em>Cumhur Erkut</em>, and Hüseyin Hacıhabiboğlu </div> <div class="periodical"> <em>In </em>, Apr 2013 </div> <div class="periodical"> 2022-12-31 Ping for eliciting </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2012</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2012wn" class="col-sm-8"> <div class="title">Product Sound Design</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> Apr 2012 </div> <div class="periodical"> #product sound design </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jylha:2012wk" class="col-sm-8"> <div class="title">Sonic Handprints: Person Identification with Hand Clapping Sounds by a Model-Based Method</div> <div class="author"> Antti Jylhä, <em>Cumhur Erkut</em>, Umut Simsekli, and <a href="https://www.linkedin.com/in/ali-taylan-cemgil-b26833131" rel="external nofollow noopener" target="_blank">Ali Taylan Cemgil</a> </div> <div class="periodical"> <em>In </em>, Apr 2012 </div> <div class="periodical"> #bayesian #biometric #hand claps #hmm #schema-sid #sid #sonic gesture </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jylha:2012ue" class="col-sm-8"> <div class="title">Rhythmic Walking Interactions with Auditory Feedback: an Exploratory Study</div> <div class="author"> Antti Jylhä, <em>Cumhur Erkut</em>, and Stefania Serafin </div> <div class="periodical"> <em>In </em>, Apr 2012 </div> <div class="periodical"> #rhythmicity #sonic interaction design #walking </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Laitinen:2012wb" class="col-sm-8"> <div class="title">Parametric time-frequency representation of spatial sound in virtual worlds</div> <div class="author"> Mikko-Ville Laitinen, Tapani Pihlajamäki, <em>Cumhur Erkut</em>, and Ville Pulkki </div> <div class="periodical"> <em>ACM Transactions on Applied Perception (TAP)</em>, Jun 2012 </div> <div class="periodical"> #DirAC #psychoacoustics #spatial </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2207216.2207219" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Directional audio coding (DirAC) is a parametric time-frequency domain method for processing spatial audio based on psychophysical assumptions and on energetic analysis of the sound field. Methods to use DirAC in spatial sound synthesis for virtual worlds are presented in this article. Formal listening tests are used to show that DirAC can be used to position and to control the spatial extent of virtual sound sources with good audio quality. It is also shown that DirAC can be used to generate reverberation for N-channel horizontal listening with only two monophonic reverberators without a prominent loss in quality when compared with quality obtained with N-channel reverberators.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jylhä.2012" class="col-sm-8"> <div class="title">Rhythmic walking interactions with auditory feedback</div> <div class="author"> Antti Jylhä, Stefania Serafin, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>Proceedings of the 7th Audio Mostly Conference on A Conference on Interaction with Sound - AM ’12</em>, Jun 2012 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2371456.2371467" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Walking is a natural rhythmic activity that has become of interest as a means of interacting with software systems such as computer games. Therefore, designing multimodal walking interactions calls for further examination. This exploratory study presents a system capable of different kinds of interactions based on varying the temporal characteristics of the output, using the sound of human walking as the input. The system either provides a direct synthesis of a walking sound based on the detected amplitude envelope of the user’s footstep sounds, or provides a continuous synthetic walking sound as a stimulus for the walking human, either with a fixed tempo or a tempo adapting to the human gait. In a pilot experiment, the different interaction modes are studied with respect to their effect on the walking tempo and the experience of the subjects. The results tentatively outline different user profiles in interacting with such a system.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2011</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jylhae-2011" class="col-sm-8"> <div class="title">Design and Evaluation of Rhythmic Interaction With an Interactive Tutoring System</div> <div class="author"> Antti Jylhä, Inger Ekman, <em>Cumhur Erkut</em>, and Koray Tahiroğlu </div> <div class="periodical"> <em>Computer Music Journal</em>, Jun 2011 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1162/comj_a_00055" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="einstein1905photoelectriceffect" class="col-sm-8"> <div class="title">Design of an audio-based mobile journey planner application</div> <div class="author"> Juho Kostiainen, <em>Cumhur Erkut</em>, and Ferran Boix Piella </div> <div class="periodical"> <em>In Proc. International Academic MindTrek Conference</em>, Sep 2011 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awards</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2181037.2181056" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://code.google.com/p/intrance-sounds/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Second prize in the category of “Innovative Interfaces” by HSL. Special Prize from Helsinki City Data Center on best usage of public data</p> </div> <div class="abstract hidden"> <p>Using public transportation is a context in which awareness of time is important. Providing information related to both time and place by means of ambient media can not only remove the need to keep looking at the time in order to depart on time but also create the feel of being in control, by knowing the time available and the progress of the journey. In the mobile context, hands and eyes are often occupied, which significantly limits the amount of information that can be provided by typical applications based on textual and graphical interfaces. In this paper, we introduce a mobile journey planner application that utilizes audio in providing useful information as well as the user experience prototyping and design process behind it.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2011vq" class="col-sm-8"> <div class="title">COST-PSD-Scientific-Report</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> Apr 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2011ua" class="col-sm-8"> <div class="title">Product Sound Design: Past, Present, Future</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, May 2011 </div> <div class="periodical"> #Product sounds #PRSD #psd #sound quality </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2011wn" class="col-sm-8"> <div class="title">Product sound design</div> <div class="author"> <em>Cumhur Erkut</em>, Stefano Delle Monache, and Davide Rocchesso </div> <div class="periodical"> <em>In </em>, May 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2011uf" class="col-sm-8"> <div class="title">Rhytmic Blueprints</div> <div class="author"> <em>Cumhur Erkut</em>, and Antti Jylhä </div> <div class="periodical"> <em>In </em>, Sep 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2011ve" class="col-sm-8"> <div class="title">Aalto STG: Product Sound Design</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> Sep 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="ProductSoundDesign:2011wn" class="col-sm-8"> <div class="title">Product Sound Design</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> Sep 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2011wz" class="col-sm-8"> <div class="title">A Structured Design and Evaluation Model with Application to Rhythmic Interaction Displays</div> <div class="author"> <em>Cumhur Erkut</em>, Antti Jylhä, and Reha Discioglu </div> <div class="periodical"> <em>In </em>, Sep 2011 </div> <div class="periodical"> #iPalmas #nime #rhythmicity </div> <div class="links"> <a href="https://doi.org/10.5281/zenodo.1178003" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jylha:2011wx" class="col-sm-8"> <div class="title">Auditory feedback in an interactive rhythmic tutoring system</div> <div class="author"> Antti Jylhä, and <em>Cumhur Erkut</em> </div> <div class="periodical"> Sep 2011 </div> <div class="periodical"> #hand claps #rhythmicity #schema-sid #sonification Current version inserts copyright to captions and removes unnecessary inspiration figures. Current version inserts copyright to captions and removes unnecessary inspiration figures. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/2095667.2095683" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We present the recent developments in the design of audio-visual feedback in iPalmas, the interactive Flamenco rhythm tutor. Based on evaluation of the original implementation, we have re-designed the interface to better support the user in learning and performing rhythmic patterns. The system measures the performance parameters of the user and provides auditory feedback on the performance with different sounds corresponding to different performance attributes. The design of these sounds is informed by several attributes derived from the evaluation. We propose informative, non-intrusive. and archetypal sounds to be used in the system.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2010</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Coutrix:2010wc" class="col-sm-8"> <div class="title">Sonic Interaction for Multitouch Displays: Design, Development, and Evaluation (SIMULS)</div> <div class="author"> Céline Coutrix, <em>Cumhur Erkut</em>, and Antti Jylhä </div> <div class="periodical"> <em></em> Feb 2010 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="DelleMonache:2010gi" class="col-sm-8"> <div class="title">Basic Exploration of Narration and Performativity for Sounding Interactive Commodities</div> <div class="author"> Stefano Delle Monache, Daniel Hug, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, Feb 2010 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-642-15841-4_8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We present an exploration in sonic interaction design, aimed at integrating the power of narrative sound design with the sonic aesthetics of a physics-based sound synthesis. The emerging process is based on interpretation, and can represent a novel tool in the ... </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jylha:2010hj" class="col-sm-8"> <div class="title">Simulation of rhythmic learning: a case study</div> <div class="author"> Antti Jylhä, <em>Cumhur Erkut</em>, Matti Pesonen, and Inger Ekman </div> <div class="periodical"> Sep 2010 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/1859799.1859819" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Simulation of human interaction with computational systems can inform their design and provide means for designing new, intelligent systems capturing some of the essence of human behavior. We describe a system simulating a situation, where a virtual tutor is teaching rhythms to a human learner. In this simulation, we virtualize the human behavior related to the learning of new rhythms. We inform the design of the system based on an experiment, in which a virtual tutor taught Flamenco hand clapping patterns to human subjects. Based on the findings on interaction with the system and learning of the patterns, we are simulating this learning situation with a virtual learning clapper. We also discuss the future work to be undertaken for more realistic, agent-based simulation of rhythmic interaction.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Simsekli:2011ks" class="col-sm-8"> <div class="title">Real-Time Recognition of Percussive Sounds by a Model-Based Method</div> <div class="author"> Umut Şimşekli, Antti Jylhä, <em>Cumhur Erkut</em> , and A T Cemgil </div> <div class="periodical"> <em>EURASIP Journal on Advances in Signal Processing</em>, Sep 2010 </div> <div class="periodical"> Recheck for bird onsets </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1155/2011/291860" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Interactive musical systems require real-time, low-latency, accurate, and reliable event detection and classification algorithms. In this paper, we introduce a model-based algorithm for detection of percussive events and test the algorithm on the detection and classification of different percussive sounds. We focus on tuning the algorithm for a good compromise between temporal precision, classification accuracy and low latency. The model is trained offline on different percussive sounds using the expectation maximization approach for learning spectral templates for each sound and is able to run online to detect and classify sounds from audio stream input by a Hidden Markov Model. Our results indicate that the approach is promising and applicable in design and development of interactive musical systems.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2009</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2009wu" class="col-sm-8"> <div class="title">Recent advances in exploring self-induced sonic interactions in the context of performing arts</div> <div class="author"> <em>Cumhur Erkut</em>, Antti Jylhä, and Inger Ekman </div> <div class="periodical"> <em>In </em>, Jun 2009 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jylha:2009uc" class="col-sm-8"> <div class="title">iPalmas - An interactive ﬂamenco rhythm machine</div> <div class="author"> Antti Jylhä, <em>Cumhur Erkut</em>, Inger Ekman, and Koray Tahiroğlu </div> <div class="periodical"> <em>In </em>, May 2009 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Anonymous:2009tx" class="col-sm-8"> <div class="title">Efﬁcient spatial sound synthesis for virtual worlds</div> <div class="author"> Ville Pulkki, Mikko-Ville Laitinen, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, May 2009 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jylha:2009gq" class="col-sm-8"> <div class="title">A hand clap interface for sonic interaction with the computer</div> <div class="author"> Antti Jylhä, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, May 2009 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/1520340.1520452" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We present a hand clapping interface for sonic interaction with the computer. The current implementation has been built on the Pure Data (PD) software. The interface makes use of the cyclic nature of hand clapping and recognition of the clap type, and enables interactive control over different applications. Three prototype applications for the interface are presented: a virtual crowd of clappers, controlling the tempo of music, and a simple sampler. Preliminary tests indicate that rather than having total control via the interface, the user negotiates with the computer to control the tempo.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut.2009" class="col-sm-8"> <div class="title">Recent advances in exploring self-induced sonic interactions in the context of performing arts</div> <div class="author"> <em>Cumhur Erkut</em>, Antti Jylhä,  KorayTahiroglu, and Inger Ekman </div> <div class="periodical"> <em></em> May 2009 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Erkut, Cumhur, Antti Jylhä, and I. Ekman. "Recent advances in exploring self-induced sonic interactions in the context of performing arts." Proceedings of the International Workshop on Haptic and Audio Interaction Design, Dresden, Germany. Vol. 29. 2009.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Salmi.2009" class="col-sm-8"> <div class="title">System for sports activity</div> <div class="author"> Timo Salmi, Tomi Salmi, Antti Jylhä, Vesa Välimäki, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Cumhur Erkut' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> May 2009 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>A system for a sports activity, which system is adapted to detect potential game contacts that include contacts between games equipment (1) and a games object (2) and/or contacts between a games object (2) and a target surface (5), and the system comprises sensor means (3a, 3b) adapted to detect vibrations caused by potential game contacts and to convert these vibrations into sensor signals. The system is adapted to - define the value of the sensor signal at several consecutive discrete time instants, - define for a sensor signal section within a time frame k that contains N sensor signal values an energy value quantity E[k] that is related to the energy of the sensor signal section, - repeat the above step of defining the energy value quantity of the sensor signal section for several consecutive time frames, and - detect a potential game contact by utilizing a detection function D that is obtained by a linear combination of energy value quantities of sensor signal sections contained in consecutive time frames by using the formula (formula), wherein K is the number of energy value quantities used in calculation and an integer equal to or greater than two, and ci is a weighting coefficient for the energy value quantity E[k-i], whereby the detection of a potential game contact in the time frame k is based on comparing the value D[k] of the detection function D corresponding to the time frame k is compared with a threshold value.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2008</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="einstein1920relativity" class="col-sm-8"> <div class="title">Proceedings of the 11th Conference on Digital Audio Effects</div> <div class="author"> J. Pakarinen, C. Erkut, H. Penttinen, and V. Välimäki </div> <div class="periodical"> Sep 2008 </div> <div class="periodical"> </div> <div class="links"> <a href="https://dafx.de" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2008" class="col-sm-8"> <div class="title">Sonic Interaction Design with Physiological Interfaces in a Workshop Setting</div> <div class="author"> <em>Cumhur Erkut</em>, Jehan-Julien Filatriau, Rémy Lehembre, and Inger Ekman </div> <div class="periodical"> <em>In </em>, Apr 2008 </div> <div class="periodical"> #CHI #chi-08 Florence, Italy </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jylha:2008tx" class="col-sm-8"> <div class="title">Sonic interactions with hand clap sounds</div> <div class="author"> Antti Jylhä, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, May 2008 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2008tb" class="col-sm-8"> <div class="title">Audio-tactile interaction at the nodes of a block-based physical sound synthesis model</div> <div class="author"> <em>Cumhur Erkut</em>, Antti Jylhä, and Ercan M Altinsoy </div> <div class="periodical"> <em>In </em>, May 2008 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jylha:2008kc" class="col-sm-8"> <div class="title">Interring the Hand Configuration from Hand Clapping Sounds</div> <div class="author"> Antti Jylhä, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, May 2008 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1111/2041-210x.12224/full" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>ABSTRACT In this paper, a technique for inferring the configuration of a clapper’s hands from a hand clapping sound is described. The method was developed based on analysis of synthetic and recorded hand clap sounds, labeled with the corresponding hand </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut.2008.The Journal of the Acoustical Society of America" class="col-sm-8"> <div class="title">Interacting with virtual musical instruments at the junction nodes</div> <div class="author"> <em>Cumhur Erkut</em>, Antti Jylhä, and Matti Karjalainen </div> <div class="periodical"> <em>The Journal of the Acoustical Society of America</em>, May 2008 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1121/1.2934447" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2007</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Olajec07:Finsig" class="col-sm-8"> <div class="title">GA-based feature selection for synchronous and asynchronous applause detection</div> <div class="author"> Jan Olajec, <em>Cumhur Erkut</em>, and Roman Jarina </div> <div class="periodical"> <em>In </em>, Aug 2007 </div> <div class="periodical"> #audience #audio indexing #clapping #classification #feature selection #genetic algorithm CD-ROM proceedings </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="eNTF07-report" class="col-sm-8"> <div class="title">Audiovisual Content Generation Controlled by Physiological Signals for Clinical and Artistic Applications</div> <div class="author"> Hannah Drayson, <em>Cumhur Erkut</em>, Jehan-Julien Filatriau, Christian Frisson, and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'Umut Gundogdu, Ben Knapp, Rémy Lehembre, Christian Mühl, Miguel Angel Ortiz Perez, Alaattin Sayin, Mohammad Soleymani, Koray Tahiroğlu, Mitchel Benovoy, Thomas Greg Corcoran' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">10 more authors</span> </div> <div class="periodical"> Aug 2007 </div> <div class="periodical"> #chi-08 #cost-sid #needkeywords #schema-sid </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>NeedAbstract</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Widmer:2007ch" class="col-sm-8"> <div class="title">Sound and Music Computing: Research Trends and Some Key Issues</div> <div class="author"> Gerhard Widmer, Davide Rocchesso, <em>Cumhur Erkut</em>, Fabien Gouyon, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Daniel Pressnitzer, Henri Penttinen, Pietro Polotti, Gualtiero Volpe' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>Journal of New Music</em>, Aug 2007 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1080/09298210701859222" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract This contribution attempts to give an overview of current research trends and open research problems in the rich field of Sound and Music Computing (SMC). To that end, the field is roughly divided into three large areas related to Sound, Music, and Interaction, respectively, and within each of these, major research trends are briefly described. In addition, for each sub-field a small number of open research (or research strategy) issues are identified that should be addressed in order to further advance the SMC field.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:qn3bvZwe" class="col-sm-8"> <div class="title">Synthesis, Control, and Hierarchical Event Modeling Algorithms for Sonic Interaction Design</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> Aug 2007 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2007ue" class="col-sm-8"> <div class="title">ClaPD: A testbed for control of multiple sound sources in interactive and participatory contexts</div> <div class="author"> <em>Cumhur Erkut</em>, and Koray Tahiroğlu </div> <div class="periodical"> <em></em> Aug 2007 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>PureData Convention 2007, (Montreal, Canada), August 21-26 2007. Online proceedings at http://artengine.ca/\textbackslashtextasciitildecatalogue-pd.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Peltola:2007dy" class="col-sm-8"> <div class="title">Synthesis of Hand Clapping Sounds</div> <div class="author"> Leevi Peltola, <em>Cumhur Erkut</em>, Perry R. Cook, and Vesa Välimäki </div> <div class="periodical"> <em>IEEE Transactions on Audio Speech and Language Processing</em>, Mar 2007 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/tasl.2006.885924" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We present two physics-based analysis, synthesis, and control systems for synthesizing hand clapping sounds. They both rely on the separation of the sound synthesis and event generation, and both are capable of producing individual hand-claps, or mimicking the asynchronous/synchronized applause of a group of clappers. The synthesis models consist of resonator filters, whose coefficients are derived from experimental measurements. The difference between these systems is mainly in the statistical event generation. While the first system allows an efficient parametric synthesis of large audiences, as well as flocking and synchronization by simple rules, the second one provides parametric extensions for synthesis of various clapping styles and enhanced control strategies. The synthesis and the control models of both systems are implemented as software running in real time at the audio sample rate, and they are available for download at at http://ccrma-www.stanford.edu/software/stk and http://www.acoustics.hut.fi/go/clapd.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Rabenstein:2007ug" class="col-sm-8"> <div class="title">Blocked-Based Physical Modeling for Digital Sound Synthesis</div> <div class="author"> Rudolf Rabenstein, Stefan Petrausch, Augusto Sarti, Giovanni De Sanctis, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Cumhur Erkut, Matti Karjalainen' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>IEEE Signal Processing Magazine</em>, Mar 2007 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/msp.2007.323263" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Block-based physical modeling is a methodology for modeling physical systems with different subsystems. It is an important concept for the physical modeling of real or virtual musical instruments where different components may be modeled according to different paradigms. Connecting systems of diverse nature in the discrete-time domain requires a common interconnection strategy. This contribution presents suitable interconnection strategies that incorporate a wide range of modeling blocks and considers the automatic implementation of block-based structures. Software environments are presented, which allow to build complex sound synthesis systems without burdening the user with problems of block compatibility.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2006</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/brownian-motion.gif" sizes="200px"></source> <img src="/assets/img/publication_preview/brownian-motion.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="brownian-motion.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="einstein1956investigations" class="col-sm-8"> <div class="title">Towards physics-based control and sound synthesis of multi-agent systems: Application to synthetic hand clapping</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In Proc. Nordic Music Technology Conference (NOMUTE)</em>, Oct 2006 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/wave-mechanics.gif" sizes="200px"></source> <img src="/assets/img/publication_preview/wave-mechanics.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="wave-mechanics.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Erkutk0v" class="col-sm-8"> <div class="title">Discrete-time modelling of musical instruments</div> <div class="author"> Vesa Välimäki, Jyri Pakarinen, <em>Cumhur Erkut</em>, and Matti Karjalainen </div> <div class="periodical"> <em>Reports on Progress in Physics</em>, Jan 2006 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1088/0034-4885/69/1/r01" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="altmetric-embed" data-badge-type="2" data-badge-popover="right" data-altmetric-id="42901613"></span> <span class="__dimensions_badge_embed__" data-doi="10.1088/0034-4885/69/1/r01" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=NPYFs5EAAAAJ&amp;citation_for_view=NPYFs5EAAAAJ:u5HHmVD_uO8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This article describes physical modelling techniques that can be used for simulating musical instruments. The methods are closely related to digital signal processing. They discretize the system with respect to time, because the aim is to run the simulation using a computer. The physics-based modelling methods can be classified as mass–spring, modal, wave digital, finite difference, digital waveguide and source–filter models. We present the basic theory and a discussion on possible extensions for each modelling technique. For some methods, a simple model example is chosen from the existing literature demonstrating a typical use of the method. For instance, in the case of the digital waveguide modelling technique a vibrating string model is discussed, and in the case of the wave digital filter technique we present a classical piano hammer model. We tackle some nonlinear and time-varying models and include new results on the digital waveguide modelling of a nonlinear string. Current trends and future directions in physical modelling of musical instruments are discussed.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2005</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2005vq" class="col-sm-8"> <div class="title">Modular interactions and hybrid models: a conceptual map for model-based sound synthesis</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, Jan 2005 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>ABSTRACT This paper presents a conceptual map for model-based sound synthesis (MBSS). This map emphasizes the role of modular interaction between sub-models and it provides a natural framework for hybrid models. The MBSS techniques are grouped ...</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Anonymous:2005um" class="col-sm-8"> <div class="title">Design and analysis of a modified kantele with increased loudness</div> <div class="author"> Henri Penttinen, <em>Cumhur Erkut</em>, and J Pölkki </div> <div class="periodical"> <em>Acta Acustica united with Acustica</em>, Jan 2005 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The kantele, an ancient plucked string instrument, belongs to the family of zithers, and it is still used in traditional folk music in Finland, Northwest Russia, and the Baltic countries. Design rules for making the kantele louder are proposed and confirming analysis results are presented. A description of the main characteristics of the traditional design of the kantele is given, so that it supports the presentation of the design rules and analysis results. The ... </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2004</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">DAFx</abbr> </div> <div id="Karjalainen:2004wu" class="col-sm-8"> <div class="title">Recent Advances in Physical Modeling with K- and W-Techniques</div> <div class="author"> Matti Karjalainen, Jyri Pakarinen, <em>Cumhur Erkut</em>, Paulo A A Esquef, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Vesa Välimäki' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Digital Audio Effects Conf. (DAFx)</em>, Sep 2004 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JASP</abbr> </div> <div id="Valimaki:2004ik" class="col-sm-8"> <div class="title">Sound Synthesis of the Harpsichord Using a Computationally Efficient Physical Model</div> <div class="author"> Henri Penttinen, Jonte Knif, Mikael Laurson, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>EURASIP Journal on Advances in Signal Processing</em>, Sep 2004 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1155/s111086570440211x" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>A sound synthesis algorithm for the harpsichord has been developed by applying the principles of digital waveguide modeling. A modification to the loss filter of the string model is introduced that allows more flexible control of decay rates of partials than is ...</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Karjalainen:2004ue" class="col-sm-8"> <div class="title">Digital Waveguides vs. Wave Digital Filters in physical modeling: Theoretical and computational aspects</div> <div class="author"> Matti Karjalainen, and <em>Cumhur Erkut</em> </div> <div class="periodical"> Sep 2004 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Digital Waveguides (DWG) are known as a highly efficient approach to physical modeling and sound synthesis of musical instruments. Recently there has been increasing interest in studying other discrete-time paradigms such as Finite Difference Time Do...</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Karjalainen:2004hp" class="col-sm-8"> <div class="title">Digital Waveguides versus Finite Difference Structures: Equivalence and Mixed Modeling</div> <div class="author"> Matti Karjalainen, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>EURASIP Journal on Advances in Signal Processing</em>, Sep 2004 </div> <div class="periodical"> #alma #asp04 #asp08-further #Audio #d9 #digital #fdtd #hybrid #Model #modeling #models #physical #processing #scattering #schema #signal #Sound #sound-source #structures #waveguides #wg </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1155/s1110865704401176" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Digital waveguides and finite difference time domain schemes have been used in physical modeling of spatially distributed systems. Both of them are known to provide exact modeling of ideal one-dimensional (1D) band-limited wave propagation, and both of them can be composed to approximate two-dimensional (2D) and three-dimensional (3D) mesh structures. Their equal capabilities in physical modeling have been shown for special cases and have been assumed to cover generalized cases as well. The ability to form mixed models by joining substructures of both classes through converter elements has been proposed recently. In this paper, we formulate a general digital signal processing (DSP)-oriented framework where the functional equivalence of these two approaches is systematically elaborated and the conditions of building mixed models are studied. An example of mixed modeling of a 2D waveguide is presented.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Karjalainen.2004.The Journal of the Acoustical Society of America" class="col-sm-8"> <div class="title">Interconnecting digital waveguides and finite difference structures: Recent advances</div> <div class="author"> Matti Karjalainen, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>The Journal of the Acoustical Society of America</em>, Sep 2004 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1121/1.4785230" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2003</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICASSP</abbr> </div> <div id="BC-ICASSP-2003" class="col-sm-8"> <div class="title">Compilation of unified physical models for efficient sound synthesis</div> <div class="author"> Matti Karjalainen, <em>Cumhur Erkut</em>, and Lauri Savioja </div> <div class="periodical"> <em>In Intl. Conf. Acoustics, Speech and Signal Processing</em>, Sep 2003 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/icassp.2003.1199999" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This paper describes a systematic approach to specification and compilation of different physical modeling schemes particularly for sound synthesis studies. First we formulate theoretically a unified way of constructing physical interaction models</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Valimaki:2003tv" class="col-sm-8"> <div class="title">Commuted waveguide synthesis of the clavichord</div> <div class="author"> M Laurson, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>Computer Music Journal</em>, Sep 2003 </div> <div class="periodical"> Cited By (since 1996): 7 Export Date: 11 November 2009 Source: Scopus </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2002</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut02:ISMA" class="col-sm-8"> <div class="title">Finite difference method vs. digital waveguide method in string instrument modeling and synthesis</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, Sep 2002 </div> <div class="periodical"> #alma #differences #finite #kw-converter #modeling #physical #physical modeling #Sound Synthesis </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Abstract The one-dimensional digital waveguides, combined with the commuted synthesis method, allow modeling and high-quality synthesis of plucked string instrument tones in a very efficient manner. However, the increasing computational power of the modern ...</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JASA</abbr> </div> <div id="Erkut02:JASA" class="col-sm-8"> <div class="title">Acoustical analysis and model-based sound synthesis of the kantele</div> <div class="author"> <em>Cumhur Erkut</em>, Matti Karjalainen, P Huang, and Vesa Välimäki </div> <div class="periodical"> <em>J Acoust Soc Am</em>, Sep 2002 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1121/1.1504858" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The five-string Finnish kantele is a traditional folk music instrument that has unique structural features, resulting in a sound of bright and reverberant timbre. This article presents an analysis of the sound generation principles in the kantele, based on measurements and ...</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut:2002tha" class="col-sm-8"> <div class="title">Aspects in analysis and model-based sound synthesis of plucked string instruments</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> <em></em> Sep 2002 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This thesis consists of an introduction and nine articles. The general framework of the articles is the analysis and synthesis of plucked string instruments. The articles document acoustical analysis and model-based sound synthesis of the tanbur, the kantele ...</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut02:AES22" class="col-sm-8"> <div class="title">Virtual strings based on a 1-D FDTD Waveguide Model: Stability, Losses, and travelling waves</div> <div class="author"> <em>Cumhur Erkut</em>, and Matti Karjalainen </div> <div class="periodical"> <em>In </em>, Jun 2002 </div> <div class="periodical"> #alma #differences #finite #modeling #physical #physical modeling #ropp #Sound Synthesis </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The one-dimensional digital waveguide structures based on finite difference time domain (FDTD) formulations provide a flexible approach for real-time sound synthesis of simple one-dimensional (1-D) structures, such as a vibrating string. This paper summarizes the basic 1-D FDTD waveguide theory, carries out the stability analysis of the model, and presents a sufficient condition for the stability. The simulation of frequency-independent losses has also been covered. The formation of the traveling waves and initialization of the 1-D FDTD waveguides are investigated. The methods shown in the paper may be used to interconnect the 1-D FDTD waveguides to other model-based sound synthesis structures, such as digital waveguides that are based on the traveling wave solution of the wave equation.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2001</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut01:ICMC" class="col-sm-8"> <div class="title">Model-based synthesis of the ud and the Renaissance lute</div> <div class="author"> <em>Cumhur Erkut</em>, M Laurson, and M Kuuskankare </div> <div class="periodical"> <em>In </em>, Jun 2001 </div> <div class="periodical"> #physical modeling #Sound Synthesis </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Abstract We describe techniques for sound synthesis of the ud and the Renaissance lute using the physical modeling approach. In the present model, novel methods have been used for the design of the loop filters, as well as for the implementation of the glissando ...</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut01:SCI" class="col-sm-8"> <div class="title">Model order selection techniques for the loop filter design of virtual string instruments</div> <div class="author"> <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In </em>, Jul 2001 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Laurson:2001tx" class="col-sm-8"> <div class="title">Methods for Modeling Realistic Playing in Acoustic Guitar Synthesis</div> <div class="author"> Mikael Laurson, <em>Cumhur Erkut</em>, Vesa Välimäki, and Mika Kuuskankare </div> <div class="periodical"> <em>Computer Music Journal</em>, Sep 2001 </div> <div class="periodical"> Cited By (since 1996): 15 Export Date: 11 November 2009 Source: Scopus </div> <div class="links"> <a href="https://doi.org/10.1162/014892601753189529" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2000</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICASSP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tanbur.png" sizes="200px"></source> <img src="/assets/img/publication_preview/tanbur.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tanbur.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="przibram1967letters" class="col-sm-8"> <div class="title">Model-Based Sound Synthesis of Tanbur, a Turkish Long-Necked Lute</div> <div class="author"> <em>Cumhur Erkut</em>, and Vesa Välimäki </div> <div class="periodical"> <em>In Proc. Intl. Conf. Acoustics, Speech and Signal Processing</em>, Jun 2000 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Erkut00:AES" class="col-sm-8"> <div class="title">Extraction of physical and expressive parameters for model-based sound synthesis of the classical guitar</div> <div class="author"> <em>Cumhur Erkut</em>, Vesa Välimäki, Matti Karjalainen, and Mikael Laurson </div> <div class="periodical"> <em>In </em>, Jun 2000 </div> <div class="periodical"> #schema </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">1999</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICMC</abbr> </div> <div id="Tolonen99:ICMC" class="col-sm-8"> <div class="title">Simulation of plucked strings exhibiting tension modulation driving force</div> <div class="author"> Tero Tolonen, <em>Cumhur Erkut</em>, Vesa Välimäki, and Matti Karjalainen </div> <div class="periodical"> <em>In Proc. Intl. Computer Music Conf.</em>, Jun 1999 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Recently, a nonlinear discrete-time model that simulates a vibrating string exhibiting tension modulation has been presented. This paper elaborates the previous model by taking into account two phenomena: 1. coupling of the tension modulation force ...</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICMC</abbr> </div> <div id="Laurson.1999" class="col-sm-8"> <div class="title">From Expressive Notation to Model-Based Sound Synthesis: a Case Study of the Acoustic Guitar</div> <div class="author"> Mikael Laurson, Jarmo Hiipakka, <em>Cumhur Erkut</em>, Matti Karjalainen, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Vesa Välimäki, Mika Kuuskankare' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proc. Intl. Computer Music Conf.</em>, Jun 1999 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The focus of this work is in modeling the unique sound and the playing practices of the acoustic guitar. The results can be applied to other plucked string instruments too. A new extended notation package is used to produce expressive control information. This tool allows the user to add to the input score instrumental expressions and tempo functions. The system includes also a rule formalism that permits further fine tuning of the computer-generated performance. A real-time synthesis engine has been developed based on earlier results in digital waveguide modeling. We also describe an analysis of vibrato in acoustic guitar tones, which provides control information for realistic synthesis.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">1997</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cem97" class="col-sm-8"> <div class="title">Calibration of physical models using artificial neural networks with application to plucked string instruments</div> <div class="author"> <a href="https://www.linkedin.com/in/ali-taylan-cemgil-b26833131" rel="external nofollow noopener" target="_blank">Ali Taylan Cemgil</a>, and <em>Cumhur Erkut</em> </div> <div class="periodical"> <em>In Proc. Intl. Symp. Musical Acoustics</em>, Jun 1997 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.researchgate.net/publication/2543549" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=NPYFs5EAAAAJ&amp;citation_for_view=NPYFs5EAAAAJ:MXK_kJrjxJIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Cumhur Erkut. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: August 08, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"Scientific Projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cuer-lab",title:"cuer lab",description:"Continual Unified Ecosystems Research Laboratory of AI, Edge, IoT, XR, and Computing Continuum",section:"Navigation",handler:()=>{window.location.href="/lab/"}},{id:"nav-teaching",title:"teaching",description:"Data Science, Machine Learning and AI Engineering courses I teach.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-10",title:"project 10",description:"A project with an introduction section",section:"Projects",handler:()=>{window.location.href="/projects/10_project/"}},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%63%75%6D%68%75%72.%65%72%6B%75%74@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0003-0750-1919","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=NPYFs5EAAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/2245025542","_blank")}},{id:"socials-publons",title:"Publons",section:"Socials",handler:()=>{window.open("https://publons.com/a/654081/","_blank")}},{id:"socials-open-science-framework",title:"Open Science Framework",section:"Socials",handler:()=>{window.open("https://osf.io/jnhxg/","_blank")}},{id:"socials-scopus",title:"Scopus",section:"Socials",handler:()=>{window.open("https://www.scopus.com/authid/detail.uri?authorId=6507065675","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/cerkut","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/cerkut","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@cumhurerkut","_blank")}},{id:"socials-gitlab",title:"GitLab",section:"Socials",handler:()=>{window.open("https://gitlab.com/cerkut","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@cerkut","_blank")}},{id:"socials-zotero",title:"Zotero",section:"Socials",handler:()=>{window.open("https://www.zotero.org/cerkut","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>